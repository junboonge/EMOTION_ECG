# -*- coding: utf-8 -*-
"""ECG_Emotion_CNN_Reduced_&_Age_Gender.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1quwM02NFCN6yq0B9LOvKgFAFhZ4NPBv-
"""

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
import os
import csv
import numpy as np
from sklearn.preprocessing import StandardScaler
import pandas as pd
import matplotlib.pyplot as plt
import pickle

# Set the folder path where CSV files are located
folder_path = "/content/drive/MyDrive/ECG_Emotion/Data"

# Initialize an empty list to store the numpy arrays
numpy_arrays = []
whole_labels = np.empty((0, 3))
whole_data = np.empty((0, 107))

test_dat = np.empty((0, 107))
train_dat = np.empty((0, 107))

label = pd.read_csv('/content/drive/MyDrive/ECG_Emotion/Label/emotion_data2.csv', index_col = 0)
label_array = np.array(label)
file_data = []

sex_array = np.array([0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 
                      0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0])

# Loop through all CSV files in the folder
for filename in os.listdir(folder_path):
    if filename.endswith(".csv"):
        file_path = os.path.join(folder_path, filename)
        with open(file_path) as csv_file:
          # Extract the numeric components of the file name
            file_nums = [int(num_str) for num_str in filename.split(".")[0].split("-")]
            # Add the file data to the list
            sub = file_nums[0]
            one = file_nums[1]
            indx = (sub-1)*18 + one - 1
            whole_labels = np.vstack((whole_labels, label_array[indx]))

            csv_reader = csv.reader(csv_file, delimiter=",")
            rows = [row for row in csv_reader]
            numpy_array = np.array(rows)
            labels_const = np.empty((0, 3))

            for i in range(len(numpy_array[3:])):
              labels_const = np.vstack((labels_const, label_array[indx]))
            numpy_array = numpy_array[3:, 1:]
            numpy_array = np.insert(numpy_array, 103, sex_array[sub - 1], axis = 1)
            numpy_array = np.concatenate((numpy_array, labels_const), axis = 1)
            
            whole_data = np.vstack((whole_data, numpy_array))

            train_dat = np.vstack((train_dat, numpy_array[:53, :]))
            test_dat = np.vstack((test_dat, numpy_array[53:, :]))

            print(filename)

whole_data = whole_data.astype(np.float32)
onadd_whole_data = np.insert(whole_data, 0, 1, axis = 1)
whole_data = onadd_whole_data

################
train_dat = train_dat.astype(np.float)
test_dat = test_dat.astype(np.float)

print(whole_data.shape)

# 데이터 섞기
whole_data_shuffled = whole_data

np.random.shuffle(whole_data_shuffled)

#Inf 파일 없애기
mask_inf = np.any(np.isinf(whole_data), axis=1)
whole_data = whole_data[~mask_inf]

mask_nan = np.any(np.isnan(whole_data), axis=1)
whole_data = whole_data[~mask_nan]

whole_size = int(len(whole_data))
train_size = int(len(whole_data)*0.7)

train_dat = whole_data[:train_size, :] #
test_dat = whole_data[train_size:, :]

mask_inf = np.any(np.isinf(onadd_train_dat), axis=1)
onadd_train_dat = onadd_train_dat[~mask_inf]

mask_inf = np.any(np.isinf(onadd_test_dat), axis=1)
onadd_test_dat = onadd_test_dat[~mask_inf]

mask_nan = np.any(np.isnan(onadd_train_dat), axis=1)
onadd_train_dat = onadd_train_dat[~mask_nan]

mask_nan = np.any(np.isnan(onadd_test_dat), axis=1)
onadd_test_dat = onadd_test_dat[~mask_nan]

print(onadd_train_dat.shape)
print(onadd_test_dat.shape)

# dataset & training set####################################################

x_train_2d = onadd_train_dat[:, :105].T
y_train_2d = onadd_train_dat[:, 105:].T

x_test_2d = onadd_test_dat[:, :105].T
y_test_2d = onadd_test_dat[:, 105:].T

print(len(y_train_2d), x_train_2d.shape)
print(len(y_test_2d), x_test_2d.shape)

# 전체 데이터를 학습시킬 때때
train_dat = whole_data[:, :]

#Inf 파일 없애기기
mask_inf_train = np.any(np.isinf(train_dat), axis=1)
train_dat = train_dat[~mask_inf_train]

mask_nan_train = np.any(np.isnan(train_dat), axis=1)
train_dat = train_dat[~mask_nan_train]

# dataset & training set
total_sam_cnt = len(whole_data[:, 0])

train_indx = len(train_dat[:, 0])
test_cnt = total_sam_cnt - train_indx

x_train_2d = train_dat[:, :105].T
y_train_2d = train_dat[:, 105:].T

x_test_2d = test_dat[:, :105].T
y_test_2d = test_dat[:, 105:].T

print(len(y_train_2d), x_train_2d.shape)
print(len(y_test_2d), x_test_2d.shape)

x_train = np.array([])
y_train = np.array([])
x_test = np.array([])
y_test = np.array([])

x_train = x_train_2d[np.newaxis, :,: ]
x_train = x_train.transpose((2, 1, 0))
y_train = y_train_2d[np.newaxis, :,: ]
y_train = y_train.transpose((2, 1, 0))

x_test = x_test_2d[np.newaxis, :,: ]
x_test = x_test.transpose((2, 1, 0))
y_test = y_test_2d[np.newaxis, :,: ]
y_test = y_test.transpose((2, 1, 0))

print(x_train.shape)

# Define the model architecture
input_shape = (105, 1)

model = tf.keras.Sequential()
model.add(tf.keras.layers.Conv1D(filters=32, kernel_size=7, activation='relu', input_shape=input_shape))
model.add(tf.keras.layers.Conv1D(filters=64, kernel_size=5, activation='relu'))
model.add(tf.keras.layers.MaxPooling1D(pool_size = 2))
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(units=256, activation='relu'))
model.add(tf.keras.layers.Dense(units=3, activation='softmax'))

# Compile the model
opt = tf.keras.optimizers.Adam(learning_rate=0.01)
model.compile(optimizer='adam', loss='categorical_crossentropy')


model.summary()
tf.keras.utils.plot_model(model)

# Testing Voice actor's data
# Load .pkl model & Data, and predict


va_ = pd.read_csv('/content/drive/MyDrive/ECG_Emotion/성우 데이터/Voice_actor_data.csv', index_col = 0)
va_data_with_r = np.array(va_)
va_data = va_data_with_r[2:, :103].T
va_data = va_data.astype('float')
va_target = va_data_with_r[2:, 103:].T

onadd_whole_data = np.insert(va_data, 0, 1, axis = 0)
va_data = onadd_whole_data

x_test = va_data[np.newaxis, :,: ]
x_test = x_test.transpose((2, 1, 0))
y_test = va_target[np.newaxis, :,: ]
y_test = y_test.transpose((2, 1, 0))

print(x_test.shape)
print(y_test.shape)

# Train the model, validation_data=(x_test, y_test)
history = model.fit(x_train, y_train, epochs=50, batch_size=32, validation_data=(x_test, y_test))

# Commented out IPython magic to ensure Python compatibility.
print(history.history['loss'])
print(history.history['val_loss'])

# %matplotlib inline

fig, loss_ax = plt.subplots()

acc_ax = loss_ax.twinx()

loss_ax.plot(history.history['loss'], 'y', label='train loss')
loss_ax.plot(history.history['val_loss'], 'r', label='val loss')

loss_ax.set_xlabel('epoch')
loss_ax.set_ylabel('loss')

loss_ax.legend(loc='upper left')
acc_ax.legend(loc='lower left')

# Predict the class label for the first sample in x_test
prediction = model.predict(x_test)

# Get the index of the predicted class
predicted_class = np.argmax(prediction)

# Confusion matrix
conf_matrix = np.zeros((3, 3))
for i in range(len(prediction)):
  conf_matrix[np.argmax(y_test[i]), np.argmax(prediction[i])] += 1

no_corr = 0
for i in range(3):
  no_corr += conf_matrix[[i], [i]]
acc = no_corr*100/len(prediction)
print("정확도: %0.2f%%"%acc)

# Set labels for the axes
labels = ['Happy', 'Sad', 'Neutral']

# Create plot and set color map
fig, ax = plt.subplots()
im = ax.imshow(conf_matrix, cmap='Blues')

# Add color bar
cbar = ax.figure.colorbar(im, ax=ax)

# Show all ticks and label them with the respective list entries
ax.set_xticks(np.arange(len(labels)))
ax.set_yticks(np.arange(len(labels)))
ax.set_xticklabels(labels)
ax.set_yticklabels(labels)

# Set the tick labels to be of same font size
plt.setp(ax.get_xticklabels(), fontsize=14)
plt.setp(ax.get_yticklabels(), fontsize=14)

# Loop over data dimensions and create text annotations
for i in range(len(labels)):
    for j in range(len(labels)):
        text = ax.text(j, i, conf_matrix[i, j],
                       ha="center", va="center", color="black", fontsize=16)

# Set the plot title and axis labels
ax.set_title("Confusion Matrix Heatmap", fontsize=18)
ax.set_xlabel("Predicted Label", fontsize=16)
ax.set_ylabel("True Label", fontsize=16)

# Adjust padding
plt.tight_layout()

fig.savefig('첫번째 모델 결과과.png')

# 모델 저장

with open('model_2_whole_trained_DREAMER.pkl', 'wb') as f:
    pickle.dump(model, f)

###########################################
onadd_train_dat = np.insert(train_dat, 0, 1, axis = 1)
onadd_test_dat = np.insert(test_dat, 0, 1, axis = 1)

print(onadd_train_dat.shape)
print(onadd_test_dat.shape)

np.random.shuffle(onadd_train_dat)
np.random.shuffle(onadd_test_dat)

print(onadd_train_dat.shape)
print(onadd_test_dat.shape)